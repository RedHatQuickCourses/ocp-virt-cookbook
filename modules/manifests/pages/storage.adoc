= Storage Manifests
:navtitle: Storage

== Overview

This page provides storage-related YAML manifests for OpenShift Virtualization, including LVMCluster configurations, StorageClass definitions, PVCs, and DataVolumes.

== LVM Storage

=== LVMCluster Configuration

After installing the LVM operator, create an LVMCluster to configure local storage:

xref:attachment$storage/lvmcluster.yaml[Download lvmcluster.yaml]

[source,yaml]
----
apiVersion: lvm.topolvm.io/v1alpha1
kind: LVMCluster
metadata:
  name: lvmcluster
  namespace: openshift-storage
spec:
  storage:
    deviceClasses:
      - name: vg1
        default: true
        deviceSelector:
          paths:
            - /dev/sdb
            - /dev/sdc
        thinPoolConfig:
          name: thin-pool-1
          sizePercent: 90
          overprovisionRatio: 10
        nodeSelector:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: In
                  values:
                    - ""
----

**Purpose:** Configures LVM storage on worker nodes using specified devices.

**Key Fields:**

* `deviceClasses[].name` - Storage class name suffix
* `default: true` - Makes this the default device class
* `paths` - Block devices to use (must be empty, no partitions)
* `sizePercent: 90` - Percentage of VG for thin pool
* `overprovisionRatio: 10` - Thin provisioning ratio

**Prerequisites:**

* LVM operator installed
* Block devices available on nodes
* Devices must be empty (no existing partitions)

**Find Available Devices:**

[source,bash,role=execute]
----
# List block devices on node
oc debug node/<node-name>
chroot /host
lsblk
----

=== Verify LVMCluster

[source,bash,role=execute]
----
# Check LVMCluster status
oc get lvmcluster -n openshift-storage

# Check LVMVolumeGroup status
oc get lvmvolumegroup -n openshift-storage

# Check created StorageClass
oc get storageclass | grep lvms

# Check topolvm pods
oc get pods -n openshift-storage -l app.kubernetes.io/name=topolvm-node
----

=== Related Tutorial

* xref:storage:lvm-operator.adoc[]
* xref:storage:lvm-troubleshooting.adoc[]

== StorageClass

=== LVM-based StorageClass

The LVM operator automatically creates StorageClass:

xref:attachment$storage/storageclass-lvms.yaml[Download storageclass-lvms.yaml]

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: lvms-vg1
provisioner: topolvm.io
parameters:
  csi.storage.k8s.io/fstype: ext4
  topolvm.io/device-class: vg1
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete
----

**Key Features:**

* `volumeBindingMode: WaitForFirstConsumer` - Delays binding until pod scheduled
* `allowVolumeExpansion: true` - Allows PVC resizing
* `reclaimPolicy: Delete` - Deletes PV when PVC deleted

=== Set Default StorageClass

[source,bash,role=execute]
----
# Remove existing default
oc patch storageclass <old-default> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'

# Set new default
oc patch storageclass lvms-vg1 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# Verify
oc get storageclass
----

**Related Tutorial:**

* xref:getting-started:default-storage-class.adoc[]

== PersistentVolumeClaim (PVC)

=== Basic PVC

xref:attachment$storage/pvc-basic.yaml[Download pvc-basic.yaml]

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vm-disk
  namespace: vms-prod
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: lvms-vg1
----

**Purpose:** Requests storage for VM disk.

**Access Modes:**

* `ReadWriteOnce` - Single node read-write (typical for VM disks)
* `ReadWriteMany` - Multi-node read-write (requires special storage)
* `ReadOnlyMany` - Multi-node read-only

=== PVC with Volume Mode Block

xref:attachment$storage/pvc-block-mode.yaml[Download pvc-block-mode.yaml]

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vm-block-disk
  namespace: vms-prod
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 100Gi
  storageClassName: lvms-vg1
----

**When to use:** For raw block devices without filesystem.

== DataVolume

DataVolumes are OpenShift Virtualization's preferred way to provision storage for VMs.

=== DataVolume from Container Image

xref:attachment$storage/datavolume-registry.yaml[Download datavolume-registry.yaml]

[source,yaml]
----
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: fedora-dv
  namespace: vms-prod
spec:
  source:
    registry:
      url: docker://quay.io/containerdisks/fedora:latest
  storage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 30Gi
    storageClassName: lvms-vg1
----

**Purpose:** Creates VM disk from container disk image.

=== DataVolume from HTTP Source

xref:attachment$storage/datavolume-http.yaml[Download datavolume-http.yaml]

[source,yaml]
----
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: rhel9-dv
  namespace: vms-prod
spec:
  source:
    http:
      url: https://example.com/rhel9.qcow2
  storage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 40Gi
    storageClassName: lvms-vg1
----

**Purpose:** Creates VM disk from qcow2 image via HTTP.

=== DataVolume from DataSource

xref:attachment$storage/datavolume-datasource.yaml[Download datavolume-datasource.yaml]

[source,yaml]
----
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: fedora-from-datasource
  namespace: vms-prod
spec:
  sourceRef:
    kind: DataSource
    name: fedora
    namespace: openshift-virtualization-os-images
  storage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 30Gi
    storageClassName: lvms-vg1
----

**Purpose:** Creates VM disk from pre-existing DataSource.

**Benefits:**

* Uses cluster-provided OS images
* Fast provisioning
* Automatically updated images

=== DataVolume Clone

xref:attachment$storage/datavolume-clone.yaml[Download datavolume-clone.yaml]

[source,yaml]
----
apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: vm-disk-clone
  namespace: vms-prod
spec:
  source:
    pvc:
      namespace: vms-prod
      name: original-vm-disk
  storage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 50Gi
    storageClassName: lvms-vg1
----

**Purpose:** Clones existing PVC to create new VM disk.

**Use cases:**

* VM templates
* Quick VM deployment
* Testing scenarios

== DataVolume Templates

DataVolumeTemplates are embedded in VM definitions for automatic provisioning.

=== VM with DataVolumeTemplate

[source,yaml]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: fedora-vm
  namespace: vms-prod
spec:
  dataVolumeTemplates:
    - apiVersion: cdi.kubevirt.io/v1beta1
      kind: DataVolume
      metadata:
        name: fedora-vm-disk
      spec:
        sourceRef:
          kind: DataSource
          name: fedora
          namespace: openshift-virtualization-os-images
        storage:
          resources:
            requests:
              storage: 30Gi
          storageClassName: lvms-vg1
  runStrategy: Always
  template:
    spec:
      domain:
        devices:
          disks:
            - disk:
                bus: virtio
              name: rootdisk
        resources:
          requests:
            memory: 4Gi
      volumes:
        - dataVolume:
            name: fedora-vm-disk
          name: rootdisk
----

**Benefits:**

* Automatic DataVolume creation with VM
* Simplified VM manifests
* Lifecycle tied to VM

== Additional Disks

=== Add Data Disk to Running VM

Create PVC:

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vm-data-disk
  namespace: vms-prod
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: lvms-vg1
----

Add to VM:

[source,bash,role=execute]
----
# Add disk to VM
virtctl addvolume <vm-name> -n vms-prod \
  --volume-name=data-disk \
  --persist \
  --serial=data001

# Verify
oc get vm <vm-name> -n vms-prod -o yaml | grep -A 5 "volumes:"
----

== Storage Verification

=== Check Storage Resources

[source,bash,role=execute]
----
# Check StorageClass
oc get storageclass

# Check PVCs
oc get pvc -A

# Check PVs
oc get pv

# Check DataVolumes
oc get datavolume -A

# Check DataSources
oc get datasource -n openshift-virtualization-os-images
----

=== Check Storage Usage

[source,bash,role=execute]
----
# Check LVM volume usage
oc get lvmvolumegroup -n openshift-storage -o yaml

# Check node storage
oc debug node/<node-name>
chroot /host
lvs
vgs
pvs
----

=== DataVolume Status

[source,bash,role=execute]
----
# Check DataVolume status
oc get dv <dv-name> -n <namespace>

# Watch DataVolume provisioning
oc get dv <dv-name> -n <namespace> -w

# Check DataVolume events
oc describe dv <dv-name> -n <namespace>
----

== Storage Best Practices

. **Use DataVolumes** for VM disks instead of direct PVCs
. **Set Default StorageClass** appropriate for your environment
. **Size Appropriately** - Consider thin provisioning ratios
. **Monitor Storage** usage to avoid exhaustion
. **Use DataSources** for common OS images
. **Test Provisioning** in development before production
. **Plan Capacity** based on overprovision ratio

== Troubleshooting

=== DataVolume Stuck in Pending

**Check:**

[source,bash,role=execute]
----
# Check DataVolume status
oc describe dv <dv-name> -n <namespace>

# Check CDI operator logs
oc logs -n openshift-cnv deployment/cdi-operator

# Check importer pod
oc get pods -n <namespace> | grep importer
oc logs <importer-pod> -n <namespace>
----

**Common Causes:**

* No default StorageClass
* Insufficient storage capacity
* Network issues downloading image
* Invalid source URL

=== PVC Not Binding

**Check:**

[source,bash,role=execute]
----
# Check PVC status
oc describe pvc <pvc-name> -n <namespace>

# Check StorageClass
oc get storageclass <storage-class>

# Check provisioner pods
oc get pods -n openshift-storage
----

**Common Causes:**

* StorageClass doesn't exist
* No available storage on nodes
* Provisioner not running
* Volume binding mode waiting for consumer

=== LVM Volume Full

**Check:**

[source,bash,role=execute]
----
# Check LVMVolumeGroup
oc get lvmvolumegroup -n openshift-storage -o yaml

# Check node LVM status
oc debug node/<node-name>
chroot /host
vgs
lvs
----

**Solutions:**

* Delete unused PVCs
* Add more disks to deviceClass
* Reduce overprovision ratio
* Expand thin pool

== Summary

Key storage manifest types:

* **LVMCluster**: Configures LVM storage operator
* **StorageClass**: Defines storage provisioner
* **PVC**: Requests storage volume
* **DataVolume**: Provisions storage for VMs
* **DataVolumeTemplate**: Embedded in VM for automatic provisioning

== See Also

* xref:storage:lvm-operator.adoc[]
* xref:storage:lvm-troubleshooting.adoc[]
* xref:getting-started:default-storage-class.adoc[]
* xref:index.adoc[Manifests Reference Overview]
* link:https://docs.openshift.com/container-platform/latest/virt/storage/virt-storage-config-overview.html[OpenShift Virtualization Storage,window=_blank]

